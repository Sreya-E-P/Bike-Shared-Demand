# Report: Predict Bike Sharing Demand with AutoGluon Solution
#### Sreya E P

## Initial Training
### What did you realize when you tried to submit your predictions? What changes were needed to the output of the predictor to submit your results?
Upon attempting to submit predictions, I realized that some of the predictions generated by the model were negative values, which Kaggle does not accept. Consequently, I needed to make changes to the output of the predictor to ensure all predictions were non-negative before submission. Specifically, I replaced all negative prediction values with 0 to meet Kaggle's submission requirements. This adjustment ensured that all predictions were valid and acceptable for submission.

### What was the top ranked model that performed?

The top performing model in terms of validation score was the WeightedEnsemble_L2 model. It achieved a validation root mean squared error (RMSE) score of -34.216675. This model was evaluated using the root_mean_squared_error metric. The prediction time on the validation set was approximately 3.575 seconds, while the training time was approximately 204.793 seconds. The marginal prediction time and marginal training time were approximately 0.001385 seconds and 0.070918 seconds, respectively. This model had a stack level of 2, indicating it was part of a stacked ensemble. Additionally, it was capable of making inferences (can_infer = True) and had a fit order of 6 in the training process.

## Exploratory data analysis and feature creation
### What did the exploratory analysis find and how did you add additional features?
Exploratory analysis is the preliminary examination of data to discover patterns, relationships, and insights. It is often used to understand the structure of a dataset and identify possible relationships between variables.

The data for year, month, day, (dayofweek) and hour were extracted as independent features separate from the datetime feature using data feature extraction. After feature extraction, the datetime feature was removed.

Feature datetime was parsed as a datetime feature to retrieve the hour information from the timestamp

The independent attributes season and weather were initially read as integer. Since these are categorical variables, they were converted to the category data type.

Another categorical feature day_type was added based on holiday and workingday feature. It was defined to effectively segregate "weekday", "weekend" and "holiday" categories.

The casual and registered features are only present in the train dataset and absent in the test data, therefore, these features were ignored during model training and after removing there, it was observed that the RMSE scores improved significantly and these independent features were highly correlated with the target variable.

### How much better did your model preform after adding additional features and why do you think that is?
Analyzing the best model scores of the initial prediction and the second prediction (after EDA), it is observed that there is an improvement of 75%.
I extracted the year, month, day, day of the week, and hour components from the datetime feature, creating separate attributes for each. This resulted in a notable enhancement in the predictive outcomes.

Furthermore, by converting specific categorical variables from integer data types to their actual categorical data types, the model's performance saw an improvement.

Lastly, I opted not to include features such as "casual" and "registered" in the training set since they are absent in the test dataset.
## Hyper parameter tuning
### How much better did your model preform after trying different hyper parameters?

Hyperparameter tuning proved advantageous as it significantly improved the model's performance compared to the initial submission. We experimented with three different configurations during the hyperparameter optimization process. While the hyperparameter-tuned models demonstrated competitive performance compared to models with exploratory data analysis (EDA) and added features, the latter exhibited notably superior results on the Kaggle test dataset.

Upon observation, we noted the following:

Although we considered prescribed settings when utilizing the autogluon package for training, the performance of hyperparameter-optimized models was suboptimal. This was because the hyperparameters were tuned with a fixed set of values provided by the user, thereby limiting the exploration options available to autogluon.
During hyperparameter optimization using autogluon, parameters such as 'time_limit' and 'presets' played crucial roles. Autogluon might fail to construct any models for a specified set of hyperparameters if the time limit is insufficient for model construction.
Hyperparameter optimization with presets like "high_quality" (with auto_stack enabled) demanded high memory usage and computational resources, making it intensive within the given time limit. Therefore, we experimented with lighter and faster preset options such as 'medium_quality' and 'optimized_for_deployment.' Ultimately, we preferred the faster and lighter "optimize_for_deployment" preset for the hyperparameter optimization routine, as other presets failed to create models using AutoGluon for the experimental configurations.
The challenge of balancing exploration and exploitation was prominent while utilizing AutoGluon with a predefined range of hyperparameters.

### If you were given more time with this dataset, where do you think you would spend more time?
Yes

### Create a table with the models you ran, the hyperparameters modified, and the kaggle score.
|model|hpo1|hpo2|hpo3|score|
|--|--|--|--|--|
|initial|prescribed_values|prescribed_values|"presets: 'high quality' (auto_stack=True)"|1.80414|
|add_features|	prescribed_values|prescribed_values	"presets: 'high quality' (auto_stack=True)"|0.46281|
|hpo|Tree-Based Models: (GBM, XT, XGB & RF)|KNN|"presets: 'optimize_for_deployment"|0.51436|

### Create a line plot showing the top model score for the three (or more) training runs during the project.


![model_train_score.png](nd009t-c1-intro-to-ml-project-starter-master\img\Screenshot 2024-04-20 231027.png)

### Create a line plot showing the top kaggle score for the three (or more) prediction submissions during the project.

![model_test_score.png](nd009t-c1-intro-to-ml-project-starter-master\img\Screenshot 2024-04-20 231027.png)

## Summary

The project extensively utilized the AutoGluon AutoML framework for Tabular Data, which automates the process of training and evaluating machine learning models. AutoGluon facilitated the creation of automated stack ensembles and individually configured regression models, allowing for tailored models for specific tasks and combining multiple models' predictions for improved performance.

Additionally, AutoGluon enabled quick prototyping of baseline models, accelerating the development and testing of initial models. The top-ranked model benefited significantly from extensive exploratory data analysis (EDA) and feature engineering, emphasizing the crucial role of human-driven insights and domain knowledge in enhancing model performance.

Automatic hyperparameter tuning, model selection, and ensembling capabilities of AutoGluon contributed to exploring and exploiting the best possible options, resulting in improved model performance. However, while hyperparameter tuning using AutoGluon showed improvements over the initial raw submission, it did not surpass the model with EDA and feature engineering alone, highlighting the greater impact of human-driven insights on model performance.

Challenges in hyperparameter tuning were observed, as it can be a complex process dependent on factors such as time limit, prescribed presets, model families, and hyperparameter range. Overall, the AutoGluon AutoML framework proved to be a valuable tool in the bike sharing demand prediction project, facilitating quick prototyping, automatic stack ensembling, and hyperparameter tuning. However, the most significant performance gains stemmed from integrating EDA and feature engineering, underscoring the importance of human expertise in building effective predictive models.
